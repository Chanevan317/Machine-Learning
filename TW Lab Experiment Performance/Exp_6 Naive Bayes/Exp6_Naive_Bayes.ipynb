{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoiS9oNgqDcE"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFu9_wVNqZie"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix, classification_report\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "models = {\n",
        "    \"GaussianNB\": GaussianNB(),\n",
        "    \"MultinomialNB\": MultinomialNB(),\n",
        "    \"BernoulliNB\": BernoulliNB()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_hso3bqrccv"
      },
      "outputs": [],
      "source": [
        "for name, model in models.items():\n",
        "    print(f\"Model: {name}\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision (macro):\", precision_score(y_test, y_pred, average='macro'))\n",
        "    print(\"Recall (macro):\", recall_score(y_test, y_pred, average='macro'))\n",
        "    print(\"F1 Score (macro):\", f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\",\n",
        "                xticklabels=iris.target_names,\n",
        "                yticklabels=iris.target_names)\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2bccm3WSrkLE",
        "outputId": "7908a62e-e282-463c-daaf-14d01d023950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   CONSOLIDATED REPORT\n",
            "           Model  Accuracy  Precision (macro)  Recall (macro)  \\\n",
            "0     GaussianNB  0.977778           0.976190        0.974359   \n",
            "1  MultinomialNB  0.955556           0.948718        0.948718   \n",
            "2    BernoulliNB  0.288889           0.096296        0.333333   \n",
            "\n",
            "   F1 Score (macro)  \n",
            "0          0.974321  \n",
            "1          0.948718  \n",
            "2          0.149425  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create summary list\n",
        "summary = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    summary.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision (macro)\": precision_score(y_test, y_pred, average='macro'),\n",
        "        \"Recall (macro)\": recall_score(y_test, y_pred, average='macro'),\n",
        "        \"F1 Score (macro)\": f1_score(y_test, y_pred, average='macro')\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "summary_df = pd.DataFrame(summary)\n",
        "\n",
        "# Print consolidated report\n",
        "print(\"   CONSOLIDATED REPORT\")\n",
        "print(summary_df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
