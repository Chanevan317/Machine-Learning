{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Name: Chan Evan Wesley\n",
        "### Enrolment: 92301733072"
      ],
      "metadata": {
        "id": "G1stnQkCvovy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Explain the impact of selecting the K value over the different natured datasets."
      ],
      "metadata": {
        "id": "6V9wSf-tkVcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Noisy datasets:\n",
        "If K is small, it will be very sensitive to noise and outliers, while with a large value of K, it will be more robust to the noise and outliers.\n",
        "\n",
        "## b. Imbalanced dataset\n",
        "In imbalanced dataset, a small value of K will be be struggling with the minority classes since the nearest neighbor will most likely be from the majority classes. If K is large, it become even more biased towards the majority classes because most of the nearest neighbors would be from theses classes.\n",
        "\n",
        "## c. Datasets with Clear Separations\n",
        "A small value of K would be effective in finding the class since the classes have clear separations, it is easier to identify nearest neighbors, while a large value of K will smooth the decision results which may blur the distincts differences between the classes and hence leading to a misclassification.\n",
        "\n",
        "## d. Datasets with Overlapping Classes\n",
        "We would get wrong classification if K is small because of the overlapping region while a large K value will give a more stable identification the class.\n",
        "\n",
        "## e. High-Dimensional Datasets\n",
        "Finding the nearest neighbors would be less meaningful since distance between points will tend to be more uniform or similar which will increase noise and classification errors. That is why we need to reduce the dimensionality but even so, if not done properly, it will lead to overfitting."
      ],
      "metadata": {
        "id": "xS7PlMn6kZt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Explain the process of applying KNN in regression problems"
      ],
      "metadata": {
        "id": "b_yaM4hHkaDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Choose the value of K\n",
        "We need to find the optimal value of K, which is the number of neighbors we will consider when we predict the class.\n",
        "\n",
        "## Step 2: Calculate the distance\n",
        "Calculate the distanc between the target and the dataset points. Euclidian distance is the most used for KNN.\n",
        "\n",
        "# Step 3: Find the nearest neighbors\n",
        "Sort the distances from smallest to highest and consider the k smallest distances.\n",
        "\n",
        "# Step 4: Decide the class\n",
        "Identify in which class the majority of the data points having the smallest distances belong to. The target point will be assigned to that class.\n",
        "In regression, we are not classifying. Instead, we are computing the average, and it will be the predicted value of the data point."
      ],
      "metadata": {
        "id": "Wv0eZcqYkgO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Can KNN be applied on the unlabelled (unsupervised learning) dataset? If yes, then how. If no, then why?"
      ],
      "metadata": {
        "id": "qR_2DmaskhK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, KNN can be applied on the unlabelled dataset since in KNN, we try to assign an unlabelled data point to an existing label in the dataset. Without a labelled dataset, we cannot give a class to that data point. Though, KNN can be applied in unsupervised learning since its nearest neighbor search can be used to generate new features for other machine learning models, to detect anomalies and outliers, and to preprocess the data in some cases, but not in classification itself."
      ],
      "metadata": {
        "id": "h3mMwoPgknB8"
      }
    }
  ]
}